---
title: "Sick dataset analysis"
author: "Wojciech Bogucki"
date: "`r format(Sys.time(), '%d/%m/%Y')`"
output:
  pdf_document:
    toc: true
    toc_depth: 2
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(OpenML)
library(dplyr)
library(tidyverse)
library(kableExtra)
library(visdat)
library(naniar)
library(DataExplorer)
library(funModeling)
library(mlr)
library(auprc)
library(mice)

```

```{r data, include = FALSE}

set.seed(245)

# download data
data <- getOMLDataSet(data.id=38)
sick <- data$data
train_idx <- read.table("../indeksy_treningowe.txt", sep=" ", header = TRUE)$x
test_idx <- setdiff(1:3772, train_idx)
sick_train <- sick[train_idx,]
sick_test <- sick[test_idx,]

```
\newpage
# Explanatory Data Analysis
## First look at raw data
Firstly, let's have a look at selected training data from the original dataset *sick*:
```{r raw data, warning=F, echo=FALSE}
kable(t(introduce(sick_train)[-9]), caption = "Features of dataset sick") %>%
  kable_styling(latex_options = "hold_position")
  kable(df_status(sick_train, print_results = FALSE), caption = "Metrics for each variable of dataset sick") %>%
    kable_styling(latex_options = "hold_position")
  
```

As we can see in the table, variable `TBG` has 100% missing values and variables `TBG_measured` and `hypopituitary` have only one unique value. Therefore, I removed this three variables from training and test datasets.
```{r removing columns, warning=FALSE, echo=FALSE}
sick_train <- sick_train %>% select(c(-TBG, -TBG_measured, -hypopituitary))
sick_test <- sick_test %>% select(c(-TBG, -TBG_measured, -hypopituitary))
```

Our target class 'Class' is very imbalanced.
```{r warning=FALSE}
freq(sick_train, input=c('Class'))
```

## Mistakes in data
When we look closely at the continuous variables in test dataset, we can observe that age variable has some big values.
```{r, echo=FALSE, fig.align='left'}
plot_histogram(sick_test[,'age'])
```

```{r, echo=FALSE}
describe(sick[,'age'])
```
Age for one observation is 455, which is obviously a mistake. To avoid such mistakes data will be filtrated. Impossible age values will be changed to missing values.
```{r change}
sick_train <- sick_train %>% mutate(age=replace(age, age>130 | age<0, NA))
sick_test <- sick_test %>% mutate(age=replace(age, age>130 | age<0, NA))
```

## Missing data
Dataset contains some missing values.
```{r missings, echo=FALSE, fig.align='left', fig.height=6}

plot_missing(sick_train,title = "Percent of missing values in training dataset")
plot_missing(sick_test,title = "Percent of missing values in test dataset")

```

## Imputation
To eliminate missing data in training set I used package `mice`.
```{r imputation, fig.align='left', fig.height=7, warning=FALSE, echo=FALSE}
sick_train_mice <- mice(sick_train, printFlag = FALSE)
sick_train_imp <- complete(sick_train_mice)
imp_met <- sick_train_mice$method[c("sex","TSH","T3","TT4","T4U","FTI")]
imp_met[imp_met=='pmm'] <- "Predictive mean matching"
imp_met[imp_met=='logreg'] <- "Logistic regression"
var_nam <- names(imp_met)
names(imp_met) <- NULL
kable(cbind(var_nam,imp_met),col.names = c('variable','imputaton method'),caption = "Impuation method for each variable")%>%
  kable_styling(latex_options = "hold_position")
plot_missing(sick_train_imp,title = "Percent of missing values \nfor each variable in traning dataset")

```
Then I performed imputation on dataset containing training data after imputation and test data.

```{r imputation test, fig.align='left', fig.height=6, warning=FALSE, echo=FALSE}
n <- nrow(sick_test)
sick_all <- rbind(sick_test, sick_train_imp)
sick_all_mice <- mice(sick_test[,-27], printFlag = FALSE)
sick_all_imp <- complete(sick_all_mice)
sick_test_imp <- cbind(sick_all_imp[1:n,], Class=sick_test$Class)
imp_met <- sick_all_mice$method[c("age","sex","TSH","T3","TT4","T4U","FTI")]
imp_met[imp_met=='pmm'] <- "Predictive mean matching"
imp_met[imp_met=='logreg'] <- "Logistic regression"
var_nam <- names(imp_met)
names(imp_met) <- NULL
kable(cbind(var_nam,imp_met),col.names = c('variable','imputaton method'),caption = "Impuation method for each variable")%>%
  kable_styling(latex_options = "hold_position")
plot_missing(sick_test_imp,title = "Percent of missing values for each variable in test dataset")
```

\newpage
# Creating models
For prediction I chose three interpretable models:

* logistic regression
* decision tree
* naive Bayes

## Training
```{r train, warning=FALSE, include=FALSE}
# logistic regression
task_logreg <- makeClassifTask("task_logreg", data=sick_train_imp, target = "Class")
learner_logreg <- makeLearner("classif.logreg", predict.type = 'prob')
model_logreg <- train(learner_logreg, task_logreg)
pred_logreg <- predict(model_logreg, newdata = sick_test_imp)
# decision trees
task_rpart<- makeClassifTask("task_rpart", data=sick_train_imp, target = "Class")
learner_rpart <- makeLearner("classif.rpart", predict.type = 'prob')
# hyperparameters tuning
rpart_ps <-  makeParamSet(
  makeIntegerParam("minsplit", lower=1, upper=50, default = 20),
  makeIntegerParam("minbucket", lower=1, upper=30),
  makeNumericParam("cp", lower=0, upper = 1, default = 0.01)
)
ctrl <-  makeTuneControlRandom(maxit = 100L)
rdesc <-  makeResampleDesc("CV", iters = 5L)
res <-  tuneParams(learner_rpart, task = task_rpart, resampling = rdesc,
                 par.set = rpart_ps, control = ctrl, measures = list(acc,auc), show.info = FALSE)
learner_rpart <-  setHyperPars(learner_rpart, minsplit=res$x$minsplit, minbucket=res$x$minbucket, cp=res$x$cp)

model_rpart <- train(learner_rpart, task_rpart)
pred_rpart <- predict(model_rpart, newdata = sick_test_imp)
# naive bayes
task_nb<- makeClassifTask("task_nb", data=sick_train_imp, target = "Class")
learner_nb <- makeLearner("classif.naiveBayes", predict.type = 'prob')
model_nb <- train(learner_nb, task_nb)
pred_nb <- predict(model_nb, newdata = sick_test_imp)


preds <- list(pred_logreg,pred_rpart,pred_nb)
```

During training I performed hyperparameters tuning on decision tree. These hyperparameteres were used:

* Decision tree:
```{r echo=FALSE}
print(res$x)
```

## Measures

Next, I calculated measures of goodness of predicton: AUC and AUPRC. I also created a comparison of ROC curve and precision-recall plot for each model.
```{r measures, warning=FALSE, echo=FALSE}
#measures
mods <- c("Logistic regression", "Decision trees","Naive Bayes")
perf_auc <- list()
perf_auprc <- list()
perf_rocr <- list()
for (i in 1:3){
  perf_auc[i] <- performance(preds[[i]],list(auc))
  perf_auprc[i] <- auprc(preds[[i]]$data$prob.sick, sick_test_imp$Class, "sick")
  pred2 <- ROCR::prediction(as.vector(preds[[i]]$data$prob.sick), as.vector(preds[[i]]$data$truth))
  perf_rocr[i] <- ROCR::performance(pred2,"tpr","fpr")
}

kable(data.frame(model=mods,auc=unlist(perf_auc),auprc=unlist(perf_auprc)), caption="Measures of goodness of prediction for each model")%>%
  kable_styling(latex_options = "hold_position")

```

```{r measures plot, echo=FALSE, fig.height=4, fig.align='left'}
# plot auc

ROCR::plot(perf_rocr[[1]], main = "ROC curve", col=1, lwd=3)
for(i in 2:3){
  ROCR::plot(perf_rocr[[i]], main = "ROC curve", col=i, add=TRUE, lwd=3)
}
legend(x=0.5,y=0.5, mods, fill=1:3)


calculate_measures_by_threshold <- function(prob, y_truth, positive_value) {
  real_positives <- sum(y_truth == positive_value)

  as.data.frame(t(sapply(seq(0, 1, length.out = 10000), function(thresh) {
    true_positives <- sum((prob >= thresh) & (y_truth == positive_value))
    det_positives <- sum(prob >= thresh)
    c(thresh = thresh,
      prec = true_positives / det_positives,
      rec = true_positives / real_positives)
  }))) %>%
    mutate(prec = ifelse(is.nan(prec), 1, prec))
}

# plot auprc
m <- data.frame()
for(i in 1:3){
 m <- rbind(m, cbind(calculate_measures_by_threshold(preds[[i]]$data$prob.sick, sick_test$Class, "sick"),model=mods[i]))
}
g <-  ggplot(aes(x = rec, y = prec, group = model, colour=model), data=m)+ geom_line()+
  xlim(c(0, 1)) + ylim(c(0, 1)) +
  ggtitle('Precision-recall plot') + xlab('recall') + ylab('precision')

g
```

# Conclusion
Even though decision tree has smaller AUC than logistic regression and naive Bayes, it has significantly higher AUPRC, which is better measure, when target variable is imbalanced like in this example.
Decion tree from package `rpart` is an example of an interpretable model. We can see the decision tree.
```{r, fig.align='left', echo=FALSE}
rpart.plot::rpart.plot(getLearnerModel(model_rpart, more.unwrap = TRUE),roundint=FALSE)
```

